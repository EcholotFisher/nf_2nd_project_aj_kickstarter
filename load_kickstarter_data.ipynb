{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import glob\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kickstarter_data(datapath):\n",
    "    '''datapath = location of csv files to be loaded'''\n",
    "    # List with the names of all the csv files in the path\n",
    "    csv_files = glob.glob(datapath+'/*.csv')\n",
    "\n",
    "    print(f'Total files: {len(csv_files)}')\n",
    "\n",
    "    # Loop through the files\n",
    "    for file_idx, csv_file in enumerate(csv_files): \n",
    "        # create dataframe from 1st csv       \n",
    "        if file_idx == 0:\n",
    "            df_ks = pd.read_csv(csv_file)\n",
    "            print(f'File number {file_idx + 1} added to dataframe')\n",
    "        else:\n",
    "            # create dataframe from idx csv\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # check files are all in same\n",
    "            if  np.all(df.columns == df_ks.columns) == False:\n",
    "                print(f'Column format of {csv_file} does not match {csv_files[0]}. Please check and try again')\n",
    "                return\n",
    "            else:\n",
    "                # append to initial dataframe                   \n",
    "                df_ks = pd.concat([df_ks, df], axis=0, ignore_index=True)       \n",
    "                print(f'File number {file_idx + 1} added to dataframe')\n",
    "    print('File import done')\n",
    "    return df_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kickstarter_data_short(datapath):\n",
    "    ''' \n",
    "    This is a version of the main function to load jsut 2 files for use in testing\n",
    "    datapath = location of csv files to be loaded\n",
    "    '''\n",
    "    # List with the names of all the csv files in the path\n",
    "    csv_files = glob.glob(datapath+'/*.csv')\n",
    "\n",
    "    print(f'Total files: {len(csv_files)}')\n",
    "\n",
    "    # Loop through the files\n",
    "    for file_idx, csv_file in enumerate(csv_files): \n",
    "        # create dataframe from 1st csv       \n",
    "        if file_idx == 0:\n",
    "            df_ks = pd.read_csv(csv_file)\n",
    "            print(f'File number {file_idx + 1} added to dataframe')\n",
    "        else:\n",
    "            # create dataframe from idx csv\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # check files are all in same\n",
    "            if  np.all(df.columns == df_ks.columns) == False:\n",
    "                print(f'Column format of {csv_file} does not match {csv_files[0]}. Please check and try again')\n",
    "                return\n",
    "            else:\n",
    "                # append to initial dataframe                   \n",
    "                df_ks = pd.concat([df_ks, df], axis=0, ignore_index=True)       \n",
    "                print(f'File number {file_idx + 1} added to dataframe')\n",
    "                # This is here to prevent more than 2 files being loaded to save time in testing\n",
    "                break\n",
    "    print('File import done')\n",
    "    return df_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_data(data):\n",
    "    ''' This function extracts specific sub fields from json files embedded in columns of a dataframe\n",
    "        data: dataframe containing column with json data'''\n",
    "    data['category_name'] = pd.DataFrame.from_dict([json.loads(data[\"category\"][i])['name'] for i in range(data.shape[0])])\n",
    "    data['category_slug'] = pd.DataFrame([json.loads(data[\"category\"][i])['slug'] for i in range(data.shape[0])])\n",
    "    # Split slug into main category and sub category\n",
    "    data[['category_main','category_sub']] = df.category_slug.str.split(pat='/', n=1, expand=True)\n",
    "    data.drop(labels = ['category','category_slug'], axis=1, inplace=True)\n",
    "    \n",
    "    print('json columns extracted')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['playing cards', 'rock', 'nonfiction', 'classical music', None,\n",
       "       'immersive', 'accessories', 'restaurants', 'world music',\n",
       "       'experimental', 'spaces', 'fine art', 'small batch', 'plays',\n",
       "       'footwear', 'sculpture', 'electronic music', 'mixed media',\n",
       "       'software', 'apparel', 'performance art', 'jewelry', 'workshops',\n",
       "       'musical', 'photobooks', 'nature', 'animals', 'ready-to-wear',\n",
       "       'places', 'country & folk', 'people', 'indie rock',\n",
       "       'graphic design', 'narrative film', 'television', 'wearables',\n",
       "       'performances', 'sound', 'video art', 'gadgets', 'printing',\n",
       "       'anthologies', 'art books', 'diy electronics', 'live games',\n",
       "       '3d printing', 'civic design', 'hardware', 'camera equipment',\n",
       "       'space exploration', 'web', 'literary spaces', 'apps', 'robots',\n",
       "       'fabrication tools', 'flight', 'taxidermy', 'vegan', 'pet fashion',\n",
       "       'poetry', 'puzzles', 'product design', 'installations', 'events',\n",
       "       'comic books', 'documentary', 'faith', 'shorts', 'translations',\n",
       "       'graphic novels', 'pottery', 'tabletop games', 'zines', 'action',\n",
       "       'public art', 'digital art', 'webcomics', 'ceramics',\n",
       "       'illustration', 'hip-hop', 'cookbooks', 'conceptual art',\n",
       "       'thrillers', 'movie theaters', \"children's books\", 'academic',\n",
       "       'video games', 'childrenswear', 'music videos', 'drinks',\n",
       "       'painting', 'fantasy', 'latin', 'romance', 'stationery', 'crochet',\n",
       "       'typography', 'comedy', \"farmer's markets\", 'animation',\n",
       "       'woodworking', 'diy', 'blues', 'gaming hardware', 'glass',\n",
       "       'architecture', 'young adult', 'r&b', 'weaving', 'horror',\n",
       "       'festivals', 'periodicals', 'knitting', 'kids', 'candles',\n",
       "       'webseries', 'drama', 'science fiction', 'jazz', 'pop', 'punk',\n",
       "       'quilts', 'fiction', 'family', 'interactive design', 'print',\n",
       "       'calendars', 'residencies', 'food trucks', 'textiles', 'audio',\n",
       "       'video', 'makerspaces', 'literary journals', 'bacon',\n",
       "       'community gardens', 'letterpress', 'mobile games', 'farms',\n",
       "       'metal', 'radio & podcasts', 'photo', 'couture', 'embroidery',\n",
       "       'chiptune'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_sub.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(data):\n",
    "    #Convert from unix time stamp to more readable time format\n",
    "    data['converted_deadline'] = pd.to_datetime(data['deadline'], unit='s')\n",
    "    data['converted_launched_at'] = pd.to_datetime(data['launched_at'], unit='s')\n",
    "    #Create project duration variable\n",
    "    data['project_duration_days'] = (data['converted_deadline'] - data['converted_launched_at']).dt.days\n",
    "    # Drop redundant columns\n",
    "    data.drop(columns=['deadline', 'launched_at'], inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(data,target='state', new_target_var='success', success_label='successful'):\n",
    "    '''\n",
    "    creates a dummy variable out of the state to be used as dependant variable\n",
    "    '''\n",
    "    #data('success') = data['state'].apply(lambda x: 1 if x == 'successful' else 0)\n",
    "    data[new_target_var] = data[target].apply(lambda x: 1 if x == success_label else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_and_features(data, target_var='success'):\n",
    "    '''\n",
    "    Function that splits dataset into target and feature dataframes\n",
    "    '''\n",
    "    #target = data['success']\n",
    "    target = data[target_var]\n",
    "    data.drop([target_var,'state'], axis = 1, inplace=True)\n",
    "    features = data\n",
    "\n",
    "    print('target and features split is done')\n",
    "    return target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def currency_conversion(data):\n",
    "    # Convert the currency of all projects to USD. \n",
    "    # We use static_usd_rate since this is what was used for usd_pledged\n",
    "    data['usd_goal'] = data['goal'] * data['static_usd_rate']\n",
    "    # drop goal and static_usd_rate to remove redundant data\n",
    "    data.drop(columns=['goal','static_usd_rate'], inplace=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    '''\n",
    "    Create new features: Blurb length\n",
    "    '''\n",
    "    data['blurb_length'] = data.blurb.apply(lambda x: len(str(x)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(data, num_columns=['usd_goal','project_duration_days','blurb_length']):\n",
    "    ''' Initialize a scaler, then apply it to the features'''\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    data[num_columns] = scaler.fit_transform(data[num_columns])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(data):\n",
    "    '''remove unnecessary columns'''\n",
    "    # Drop due to many missing values\n",
    "    data.drop(columns = ['friends', 'is_backing', 'is_starred', 'permissions'], inplace=True)\n",
    "    # Some Json strings varariables with unusable or already used data\n",
    "    data.drop(columns = ['creator', 'location', 'photo', 'profile', 'slug', 'urls'], inplace=True)\n",
    "    # Columns that are not specific to the campaign or are redundant or are technical data unrelated to campaign\n",
    "    data.drop(columns = ['created_at','currency', 'currency_symbol', 'currency_trailing_code', \n",
    "                     'current_currency', 'disable_communication',\n",
    "                     'is_starrable', 'source_url', 'spotlight', 'staff_pick', \n",
    "                     'usd_type', 'state_changed_at','fx_rate'], inplace=True)\n",
    "    # drop columns due to being linked to dependent variable which would not be known in advance\n",
    "    data.drop(columns = ['backers_count', 'converted_pledged_amount', 'pledged', 'usd_pledged','id'], inplace=True) # to be checked 'backers_count'\n",
    "    # drop columns that are not used                \n",
    "    data.drop(columns = ['blurb', 'name', 'converted_deadline', 'converted_launched_at','category_name'], inplace=True) #'category_slug'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split_kickstarter(features, target, test_size = 0.2, random_state = 0):\n",
    "    '''\n",
    "    Split data into train and test sets based on features and target dataframes.\n",
    "    Shows results of the split and returns four dataframes\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = test_size, random_state = random_state)\n",
    "    # Show the results of the split\n",
    "    print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "    print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_gini(X_train, y_train): \n",
    "  \n",
    "    # Creating the decision tree classifier object \n",
    "    clf_tree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "            max_depth=3, min_samples_leaf=5) \n",
    "    # Performing training \n",
    "    clf_tree.fit(X_train, y_train) \n",
    "    return clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions \n",
    "def prediction(X_test, clf_object): \n",
    "  \n",
    "    # Predicton on test data with model trained using either giniIndex\n",
    "    y_pred = clf_object.predict(X_test) \n",
    "    print(\"Predicted values:\\n\") \n",
    "    print(y_pred) \n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(y_test, y_pred): \n",
    "      \n",
    "    print(\"Confusion Matrix: \\n\", \n",
    "    confusion_matrix(y_test, y_pred)) \n",
    "      \n",
    "    print (\"Accuracy : \\n\", \n",
    "    accuracy_score(y_test, y_pred)*100) \n",
    "      \n",
    "    print(\"Report : \\n\", \n",
    "    classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train[:sample_size],y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train[:300],predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test,predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples\n",
    "    results['f_train'] = fbeta_score(y_train[:300],predictions_train,beta=0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set\n",
    "    results['f_test'] = fbeta_score(y_test,predictions_test,beta=0.5)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 56\n",
      "File number 1 added to dataframe\n",
      "File number 2 added to dataframe\n",
      "File number 3 added to dataframe\n",
      "File number 4 added to dataframe\n",
      "File number 5 added to dataframe\n",
      "File number 6 added to dataframe\n",
      "File number 7 added to dataframe\n",
      "File number 8 added to dataframe\n",
      "File number 9 added to dataframe\n",
      "File number 10 added to dataframe\n",
      "File number 11 added to dataframe\n",
      "File number 12 added to dataframe\n",
      "File number 13 added to dataframe\n",
      "File number 14 added to dataframe\n",
      "File number 15 added to dataframe\n",
      "File number 16 added to dataframe\n",
      "File number 17 added to dataframe\n",
      "File number 18 added to dataframe\n",
      "File number 19 added to dataframe\n",
      "File number 20 added to dataframe\n",
      "File number 21 added to dataframe\n",
      "File number 22 added to dataframe\n",
      "File number 23 added to dataframe\n",
      "File number 24 added to dataframe\n",
      "File number 25 added to dataframe\n",
      "File number 26 added to dataframe\n",
      "File number 27 added to dataframe\n",
      "File number 28 added to dataframe\n",
      "File number 29 added to dataframe\n",
      "File number 30 added to dataframe\n",
      "File number 31 added to dataframe\n",
      "File number 32 added to dataframe\n",
      "File number 33 added to dataframe\n",
      "File number 34 added to dataframe\n",
      "File number 35 added to dataframe\n",
      "File number 36 added to dataframe\n",
      "File number 37 added to dataframe\n",
      "File number 38 added to dataframe\n",
      "File number 39 added to dataframe\n",
      "File number 40 added to dataframe\n",
      "File number 41 added to dataframe\n",
      "File number 42 added to dataframe\n",
      "File number 43 added to dataframe\n",
      "File number 44 added to dataframe\n",
      "File number 45 added to dataframe\n",
      "File number 46 added to dataframe\n",
      "File number 47 added to dataframe\n",
      "File number 48 added to dataframe\n",
      "File number 49 added to dataframe\n",
      "File number 50 added to dataframe\n",
      "File number 51 added to dataframe\n",
      "File number 52 added to dataframe\n",
      "File number 53 added to dataframe\n",
      "File number 54 added to dataframe\n",
      "File number 55 added to dataframe\n",
      "File number 56 added to dataframe\n",
      "File import done\n",
      "json columns extracted\n",
      "target and features split is done\n",
      "Training set has 161528 samples.\n",
      "Testing set has 40383 samples.\n",
      "Predicted values:\n",
      "\n",
      "[0 1 0 ... 0 1 0]\n",
      "Confusion Matrix: \n",
      " [[ 5681 11218]\n",
      " [ 2742 20742]]\n",
      "Accuracy : \n",
      " 65.43099819230864\n",
      "Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.34      0.45     16899\n",
      "           1       0.65      0.88      0.75     23484\n",
      "\n",
      "    accuracy                           0.65     40383\n",
      "   macro avg       0.66      0.61      0.60     40383\n",
      "weighted avg       0.66      0.65      0.62     40383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MAIN SCRIPT =============================================================\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read data\n",
    "\n",
    "    df = load_kickstarter_data('kickstarter/data')\n",
    "    \n",
    "    # Extract category data from json\n",
    "    df = extract_json_data(df)\n",
    "    \n",
    "    # drop campaigns that are still ongoing\n",
    "    df = df[df.state != 'live'] # change this to sucessful and \n",
    "    \n",
    "    # convert unix timestamps and calculate campaign duration\n",
    "    df = get_duration(df)\n",
    "    \n",
    "    # Get blurb length\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    #create goal data in single currency\n",
    "    df = currency_conversion(df)\n",
    "    \n",
    "    # encode target variable 'state' to numerical values, success is 1 all others are fail and 0\n",
    "    df = get_target(df,target='state', new_target_var='success', success_label='successful')\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    df=drop_columns(df)\n",
    "    df.head()\n",
    "\n",
    "    # Split the data into features and target label\n",
    "    target, features = get_target_and_features(df)\n",
    "\n",
    "    # split categorical columns into dummies\n",
    "    features = pd.get_dummies(features, columns=['country', 'category_main','category_sub'], drop_first=True) #Avoid dummy trap   \n",
    "\n",
    "    # Clean and augment data\n",
    "    # scale numerical features\n",
    "    num_columns = ['usd_goal','project_duration_days']\n",
    "    features = scale_features(features, num_columns)\n",
    "    \n",
    "    # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = test_train_split_kickstarter(features, target, test_size = 0.2, random_state = 0)\n",
    "    \n",
    "    # Fit a simple decision tree first\n",
    "    clf_gini = train_using_gini(X_train, y_train) \n",
    "    \n",
    "    # Create predictions using simple model\n",
    "    y_pred = prediction(X_test, clf_object=clf_gini)\n",
    "    \n",
    "    # show results\n",
    "    cal_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier trained on 1615 samples.\n",
      "DecisionTreeClassifier trained on 16153 samples.\n",
      "DecisionTreeClassifier trained on 161528 samples.\n",
      "AdaBoostClassifier trained on 1615 samples.\n",
      "AdaBoostClassifier trained on 16153 samples.\n",
      "AdaBoostClassifier trained on 161528 samples.\n",
      "AdaBoostClassifier trained on 1615 samples.\n",
      "AdaBoostClassifier trained on 16153 samples.\n",
      "AdaBoostClassifier trained on 161528 samples.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from time import time\n",
    "\n",
    "# TODO: Initialize the three models, the random states are set to 101 so we know how to reproduce the model later\n",
    "clf_A = DecisionTreeClassifier(random_state=101)\n",
    "clf_B = AdaBoostClassifier(random_state = 101) #SVC(random_state = 101)\n",
    "clf_C = AdaBoostClassifier(random_state = 101)\n",
    "\n",
    "# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "samples_1 = int(round(len(X_train) / 100))\n",
    "samples_10 = int(round(len(X_train) / 10))\n",
    "samples_100 = int(round(len(X_train) / 1))\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>0.041520</td>\n",
       "      <td>0.506177</td>\n",
       "      <td>2.927807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>0.040160</td>\n",
       "      <td>0.054694</td>\n",
       "      <td>0.051908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_test</th>\n",
       "      <td>0.638536</td>\n",
       "      <td>0.682168</td>\n",
       "      <td>0.721888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_test</th>\n",
       "      <td>0.685787</td>\n",
       "      <td>0.726112</td>\n",
       "      <td>0.758332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1%       10%       50%\n",
       "train_time  0.041520  0.506177  2.927807\n",
       "pred_time   0.040160  0.054694  0.051908\n",
       "acc_train   1.000000  1.000000  0.983333\n",
       "acc_test    0.638536  0.682168  0.721888\n",
       "f_train     1.000000  1.000000  0.990566\n",
       "f_test      0.685787  0.726112  0.758332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>0.548806</td>\n",
       "      <td>66.956755</td>\n",
       "      <td>1766.099654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>11.065032</td>\n",
       "      <td>167.028664</td>\n",
       "      <td>724.809085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_train</th>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.573333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_test</th>\n",
       "      <td>0.581532</td>\n",
       "      <td>0.581532</td>\n",
       "      <td>0.582126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_train</th>\n",
       "      <td>0.626822</td>\n",
       "      <td>0.626822</td>\n",
       "      <td>0.626822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_test</th>\n",
       "      <td>0.634648</td>\n",
       "      <td>0.634648</td>\n",
       "      <td>0.634981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1%         10%          50%\n",
       "train_time   0.548806   66.956755  1766.099654\n",
       "pred_time   11.065032  167.028664   724.809085\n",
       "acc_train    0.573333    0.573333     0.573333\n",
       "acc_test     0.581532    0.581532     0.582126\n",
       "f_train      0.626822    0.626822     0.626822\n",
       "f_test       0.634648    0.634648     0.634981"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>0.172233</td>\n",
       "      <td>1.247502</td>\n",
       "      <td>7.306491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>0.968681</td>\n",
       "      <td>0.822130</td>\n",
       "      <td>0.783684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_train</th>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_test</th>\n",
       "      <td>0.713890</td>\n",
       "      <td>0.731397</td>\n",
       "      <td>0.722086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_train</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.780543</td>\n",
       "      <td>0.726496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_test</th>\n",
       "      <td>0.751910</td>\n",
       "      <td>0.766942</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1%       10%       50%\n",
       "train_time  0.172233  1.247502  7.306491\n",
       "pred_time   0.968681  0.822130  0.783684\n",
       "acc_train   0.753333  0.753333  0.696667\n",
       "acc_test    0.713890  0.731397  0.722086\n",
       "f_train     0.789474  0.780543  0.726496\n",
       "f_test      0.751910  0.766942  0.750000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Printing out the values\n",
    "for i in results.items():\n",
    "    print (i[0])\n",
    "    display(pd.DataFrame(i[1]).rename(columns={0:'1%', 1:'10%', 2:'50%'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['blurb_length'] = df.blurb.apply(lambda x: len((x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of dataframes\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.dtype)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category_slug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['success'] = df.state.apply(lambda x: True if x == 'successful' else False)\n",
    "df['match'] = df['success'] == df['reached_goal']\n",
    "df.query('state == \"successful\"').match.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff for figureing out exchange rates\n",
    "df['implied_fx_rate'] = df['usd_pledged'] / df['pledged']\n",
    "df[['usd_pledged','implied_fx_rate','fx_rate','static_usd_rate']].head(10)\n",
    "df['usd_goal'] = df['goal'] * df['static_usd_rate']\n",
    "df['reached_goal'] = df['usd_goal'] < df['usd_pledged']\n",
    "df['success'] = df.state.apply(lambda x: True if x == 'successful' else False)\n",
    "df['match'] = df['success'] == df['reached_goal']\n",
    "df.query('state == \"successful\" & usd_type == \"international\"').match.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf_2nd_project] *",
   "language": "python",
   "name": "conda-env-nf_2nd_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
