{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kickstarter_data(datapath):\n",
    "    '''datapath = location of csv files to be loaded'''\n",
    "    # List with the names of all the csv files in the path\n",
    "    csv_files = glob.glob(datapath+'/*.csv')\n",
    "\n",
    "    print(f'Total files: {len(csv_files)}')\n",
    "\n",
    "    # Loop through the files\n",
    "    for file_idx, csv_file in enumerate(csv_files): \n",
    "        # create dataframe from 1st csv       \n",
    "        if file_idx == 0:\n",
    "            df_ks = pd.read_csv(csv_file)\n",
    "            print(f'File number {file_idx + 1} added to dataframe')\n",
    "        else:\n",
    "            # create dataframe from idx csv\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # check files are all in same\n",
    "            if  np.all(df.columns == df_ks.columns) == False:\n",
    "                print(f'Column format of {csv_file} does not match {csv_files[0]}. Please check and try again')\n",
    "                return\n",
    "            else:\n",
    "                # append to initial dataframe                   \n",
    "                df_ks = pd.concat([df_ks, df], axis=0, ignore_index=True)       \n",
    "                print(f'File number {file_idx + 1} added to dataframe')\n",
    "    print('File import done')\n",
    "    return df_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 56\n",
      "File number 1 added to dataframe\n",
      "File number 2 added to dataframe\n",
      "File number 3 added to dataframe\n",
      "File number 4 added to dataframe\n",
      "File number 5 added to dataframe\n",
      "File number 6 added to dataframe\n",
      "File number 7 added to dataframe\n",
      "File number 8 added to dataframe\n",
      "File number 9 added to dataframe\n",
      "File number 10 added to dataframe\n",
      "File number 11 added to dataframe\n",
      "File number 12 added to dataframe\n",
      "File number 13 added to dataframe\n",
      "File number 14 added to dataframe\n",
      "File number 15 added to dataframe\n",
      "File number 16 added to dataframe\n",
      "File number 17 added to dataframe\n",
      "File number 18 added to dataframe\n",
      "File number 19 added to dataframe\n",
      "File number 20 added to dataframe\n",
      "File number 21 added to dataframe\n",
      "File number 22 added to dataframe\n",
      "File number 23 added to dataframe\n",
      "File number 24 added to dataframe\n",
      "File number 25 added to dataframe\n",
      "File number 26 added to dataframe\n",
      "File number 27 added to dataframe\n",
      "File number 28 added to dataframe\n",
      "File number 29 added to dataframe\n",
      "File number 30 added to dataframe\n",
      "File number 31 added to dataframe\n",
      "File number 32 added to dataframe\n",
      "File number 33 added to dataframe\n",
      "File number 34 added to dataframe\n",
      "File number 35 added to dataframe\n",
      "File number 36 added to dataframe\n",
      "File number 37 added to dataframe\n",
      "File number 38 added to dataframe\n",
      "File number 39 added to dataframe\n",
      "File number 40 added to dataframe\n",
      "File number 41 added to dataframe\n",
      "File number 42 added to dataframe\n",
      "File number 43 added to dataframe\n",
      "File number 44 added to dataframe\n",
      "File number 45 added to dataframe\n",
      "File number 46 added to dataframe\n",
      "File number 47 added to dataframe\n",
      "File number 48 added to dataframe\n",
      "File number 49 added to dataframe\n",
      "File number 50 added to dataframe\n",
      "File number 51 added to dataframe\n",
      "File number 52 added to dataframe\n",
      "File number 53 added to dataframe\n",
      "File number 54 added to dataframe\n",
      "File number 55 added to dataframe\n",
      "File number 56 added to dataframe\n",
      "File import done\n"
     ]
    }
   ],
   "source": [
    "df = load_kickstarter_data('kickstarter/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_data(data):\n",
    "    ''' This function extracts specific sub fields from json files embedded in columns of a dataframe\n",
    "        data: dataframe containing column with json data'''\n",
    "    data['category_slug'] = pd.DataFrame([json.loads(data[\"category\"][i])['slug'] for i in range(data.shape[0])])\n",
    "    data['category_name'] = pd.DataFrame([json.loads(data[\"category\"][i])['name'] for i in range(data.shape[0])])\n",
    "#   data['location_name'] = pd.DataFrame([json.loads(data[\"location\"][i])['name'] for i in range(data.shape[0])])\n",
    "    return data\n",
    "#df2 = extract_json_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['backers_count', 'blurb', 'category', 'converted_pledged_amount',\n",
      "       'country', 'created_at', 'creator', 'currency', 'currency_symbol',\n",
      "       'currency_trailing_code', 'current_currency', 'deadline',\n",
      "       'disable_communication', 'friends', 'fx_rate', 'goal', 'id',\n",
      "       'is_backing', 'is_starrable', 'is_starred', 'launched_at', 'location',\n",
      "       'name', 'permissions', 'photo', 'pledged', 'profile', 'slug',\n",
      "       'source_url', 'spotlight', 'staff_pick', 'state', 'state_changed_at',\n",
      "       'static_usd_rate', 'urls', 'usd_pledged', 'usd_type', 'category_slug',\n",
      "       'category_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.DataFrame.from_dict(dict_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        name        slug  position    color  \\\n",
      "web  13  Journalism  journalism        10  1228010   \n",
      "\n",
      "                                                  urls  \n",
      "web  {'discover': 'http://www.kickstarter.com/disco...  \n"
     ]
    }
   ],
   "source": [
    "#data['category_slug'] = pd.DataFrame([json.loads(data[\"category\"][i])[\"slug\"] for i in range(data.shape[0])])\n",
    "json_columns = ['category'] #, 'creator', 'urls', 'location']\n",
    "for col in json_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        dict_json = json.loads(df[col][i])\n",
    "    df_json = pd.DataFrame.from_dict(dict_json) \n",
    "    print(df_json.head())\n",
    "#    df0 = pd.merge(left=df0, right=df_json, left_index=True, right_index=True)       \n",
    "\n",
    "#df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-b9582da371e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "df_json.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MAIN SCRIPT =============================================================\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read data\n",
    "    df = load_kickstarter_data('kickstarter/data')\n",
    "\n",
    "    # Clean data\n",
    "    df = extract_json_data(df)\n",
    "    # Split into training and test set\n",
    "    \n",
    "    # Fit a simple decision tree first\n",
    "    \n",
    "    # Fit Adaboost classifier using a decision tree as base estimator\n",
    "    # Test with different number of iterations\n",
    "    # Append the score output from each iteration of the adboost function\n",
    "    \n",
    "    # Compare error rate vs number of iterations\n",
    "    #   plot_error_rate(er_train, er_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf_2nd_project] *",
   "language": "python",
   "name": "conda-env-nf_2nd_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
