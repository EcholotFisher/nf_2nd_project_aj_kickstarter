{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import glob\n",
    "import json\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "RSEED = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kickstarter_data(datapath):\n",
    "    '''datapath = location of csv files to be loaded'''\n",
    "    # List with the names of all the csv files in the path\n",
    "    csv_files = glob.glob(datapath+'/*.csv')\n",
    "\n",
    "    print(f'Total files: {len(csv_files)}')\n",
    "\n",
    "    # Loop through the files\n",
    "    for file_idx, csv_file in enumerate(csv_files): \n",
    "        # create dataframe from 1st csv       \n",
    "        if file_idx == 0:\n",
    "            df_ks = pd.read_csv(csv_file)\n",
    "            print(f'File number {file_idx + 1} added to dataframe')\n",
    "        else:\n",
    "            # create dataframe from idx csv\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # check files are all in same\n",
    "            if  np.all(df.columns == df_ks.columns) == False:\n",
    "                print(f'Column format of {csv_file} does not match {csv_files[0]}. Please check and try again')\n",
    "                return\n",
    "            else:\n",
    "                # append to initial dataframe                   \n",
    "                df_ks = pd.concat([df_ks, df], axis=0, ignore_index=True)       \n",
    "                print(f'File number {file_idx + 1} added to dataframe')\n",
    "    print('File import done')\n",
    "    return df_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kickstarter_data_short(datapath):\n",
    "    ''' \n",
    "    This is a version of the main function to load jsut 2 files for use in testing\n",
    "    datapath = location of csv files to be loaded\n",
    "    '''\n",
    "    # List with the names of all the csv files in the path\n",
    "    csv_files = glob.glob(datapath+'/*.csv')\n",
    "\n",
    "    print(f'Total files: {len(csv_files)}')\n",
    "\n",
    "    # Loop through the files\n",
    "    for file_idx, csv_file in enumerate(csv_files): \n",
    "        # create dataframe from 1st csv       \n",
    "        if file_idx == 0:\n",
    "            df_ks = pd.read_csv(csv_file)\n",
    "            print(f'File number {file_idx + 1} added to dataframe')\n",
    "        else:\n",
    "            # create dataframe from idx csv\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # check files are all in same\n",
    "            if  np.all(df.columns == df_ks.columns) == False:\n",
    "                print(f'Column format of {csv_file} does not match {csv_files[0]}. Please check and try again')\n",
    "                return\n",
    "            else:\n",
    "                # append to initial dataframe                   \n",
    "                df_ks = pd.concat([df_ks, df], axis=0, ignore_index=True)       \n",
    "                print(f'File number {file_idx + 1} added to dataframe')\n",
    "                # This is here to prevent more than 2 files being loaded to save time in testing\n",
    "                break\n",
    "    print('File import done')\n",
    "    return df_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_data(data):\n",
    "    ''' This function extracts specific sub fields from json files embedded in columns of a dataframe\n",
    "        data: dataframe containing column with json data'''\n",
    "    data['category_name'] = pd.DataFrame.from_dict([json.loads(data[\"category\"][i])['name'] for i in range(data.shape[0])])\n",
    "    data['category_slug'] = pd.DataFrame([json.loads(data[\"category\"][i])['slug'] for i in range(data.shape[0])])\n",
    "    # Split slug into main category and sub category\n",
    "    data[['category_main','category_sub']] = df.category_slug.str.split(pat='/', n=1, expand=True)\n",
    "    data.drop(labels = ['category','category_slug'], axis=1, inplace=True)\n",
    "    \n",
    "    print('json columns extracted')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(data):\n",
    "    #Convert from unix time stamp to more readable time format\n",
    "    data['converted_deadline'] = pd.to_datetime(data['deadline'], unit='s')\n",
    "    data['converted_launched_at'] = pd.to_datetime(data['launched_at'], unit='s')\n",
    "    #Create project duration variable\n",
    "    data['project_duration_days'] = (data['converted_deadline'] - data['converted_launched_at']).dt.days\n",
    "    # Drop redundant columns\n",
    "    data.drop(columns=['deadline', 'launched_at'], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(data,target='state', new_target_var='success', success_label='successful'):\n",
    "    '''\n",
    "    creates a dummy variable out of the state to be used as dependant variable\n",
    "    '''\n",
    "    #data('success') = data['state'].apply(lambda x: 1 if x == 'successful' else 0)\n",
    "    data[new_target_var] = data[target].apply(lambda x: 1 if x == success_label else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_and_features(data, target_var='success'):\n",
    "    '''\n",
    "    Function that splits dataset into target and feature dataframes\n",
    "    '''\n",
    "    #target = data['success']\n",
    "    target = data[target_var]\n",
    "    data.drop([target_var,'state'], axis = 1, inplace=True)\n",
    "    features = data\n",
    "\n",
    "    print('target and features split is done')\n",
    "    return target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def currency_conversion(data):\n",
    "    # Convert the currency of all projects to USD. \n",
    "    # We use static_usd_rate since this is what was used for usd_pledged\n",
    "    data['usd_goal'] = data['goal'] * data['static_usd_rate']\n",
    "    # drop goal and static_usd_rate to remove redundant data\n",
    "    data.drop(columns=['goal','static_usd_rate'], inplace=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    '''\n",
    "    Create new features: Blurb length\n",
    "    '''\n",
    "    data['blurb_length'] = data.blurb.apply(lambda x: len(str(x)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_skew(data, skewed=['usd_goal']):\n",
    "    '''Log-transform the skewed features'''\n",
    "    data[skewed] = data[skewed].apply(lambda x: np.log(x + 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(data, num_columns=['usd_goal','project_duration_days','blurb_length']):\n",
    "    ''' Initialize a scaler, then apply it to the features'''    \n",
    "    scaler = MinMaxScaler()\n",
    "    data[num_columns] = scaler.fit_transform(data[num_columns])    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(data):\n",
    "    '''remove unnecessary columns'''\n",
    "    # Drop due to many missing values\n",
    "    data.drop(columns = ['friends', 'is_backing', 'is_starred', 'permissions'], inplace=True)\n",
    "    # Some Json strings varariables with unusable or already used data\n",
    "    data.drop(columns = ['creator', 'location', 'photo', 'profile', 'slug', 'urls'], inplace=True)\n",
    "    # Columns that are not specific to the campaign or are redundant or are technical data unrelated to campaign\n",
    "    data.drop(columns = ['created_at','currency', 'currency_symbol', 'currency_trailing_code', \n",
    "                     'current_currency', 'disable_communication',\n",
    "                     'is_starrable', 'source_url', 'spotlight', 'staff_pick', \n",
    "                     'usd_type', 'state_changed_at','fx_rate'], inplace=True)\n",
    "    # drop columns due to being linked to dependent variable which would not be known in advance\n",
    "    data.drop(columns = ['backers_count', 'converted_pledged_amount', 'pledged', 'usd_pledged','id'], inplace=True) # to be checked 'backers_count'\n",
    "    # drop columns that are not used                \n",
    "    data.drop(columns = ['blurb', 'name', 'converted_deadline', 'converted_launched_at','category_name'], inplace=True) #'category_slug'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split_kickstarter(features, target, test_size = 0.2, random_state = RSEED):\n",
    "    '''\n",
    "    Split data into train and test sets based on features and target dataframes.\n",
    "    Shows results of the split and returns four dataframes\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = test_size, random_state = random_state)\n",
    "    # Show the results of the split\n",
    "    print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "    print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_gini(X_train, y_train):  \n",
    "    # Creating the decision tree classifier object \n",
    "    clf_tree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "            max_depth=3, min_samples_leaf=5) \n",
    "    # Performing training \n",
    "    clf_tree.fit(X_train, y_train) \n",
    "    return clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions \n",
    "def prediction(X_test, clf_object): \n",
    "  \n",
    "    # Predicton on test data with model trained using either giniIndex\n",
    "    y_pred = clf_object.predict(X_test) \n",
    "    print(\"Predicted values:\\n\") \n",
    "    print(y_pred) \n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(y_test, y_pred): \n",
    "      \n",
    "    print(\"Confusion Matrix: \\n\", \n",
    "    confusion_matrix(y_test, y_pred)) \n",
    "      \n",
    "    print (\"Accuracy : \\n\", \n",
    "    accuracy_score(y_test, y_pred)*100) \n",
    "      \n",
    "    print(\"Report : \\n\", \n",
    "    classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train[:sample_size],y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train[:300],predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test,predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples\n",
    "    results['f_train'] = fbeta_score(y_train[:300],predictions_train,beta=0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set\n",
    "    results['f_test'] = fbeta_score(y_test,predictions_test,beta=0.5)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 56\n",
      "File number 1 added to dataframe\n",
      "File number 2 added to dataframe\n",
      "File number 3 added to dataframe\n",
      "File number 4 added to dataframe\n",
      "File number 5 added to dataframe\n",
      "File number 6 added to dataframe\n",
      "File number 7 added to dataframe\n",
      "File number 8 added to dataframe\n",
      "File number 9 added to dataframe\n",
      "File number 10 added to dataframe\n",
      "File number 11 added to dataframe\n",
      "File number 12 added to dataframe\n",
      "File number 13 added to dataframe\n",
      "File number 14 added to dataframe\n",
      "File number 15 added to dataframe\n",
      "File number 16 added to dataframe\n",
      "File number 17 added to dataframe\n",
      "File number 18 added to dataframe\n",
      "File number 19 added to dataframe\n",
      "File number 20 added to dataframe\n",
      "File number 21 added to dataframe\n",
      "File number 22 added to dataframe\n",
      "File number 23 added to dataframe\n",
      "File number 24 added to dataframe\n",
      "File number 25 added to dataframe\n",
      "File number 26 added to dataframe\n",
      "File number 27 added to dataframe\n",
      "File number 28 added to dataframe\n",
      "File number 29 added to dataframe\n",
      "File number 30 added to dataframe\n",
      "File number 31 added to dataframe\n",
      "File number 32 added to dataframe\n",
      "File number 33 added to dataframe\n",
      "File number 34 added to dataframe\n",
      "File number 35 added to dataframe\n",
      "File number 36 added to dataframe\n",
      "File number 37 added to dataframe\n",
      "File number 38 added to dataframe\n",
      "File number 39 added to dataframe\n",
      "File number 40 added to dataframe\n",
      "File number 41 added to dataframe\n",
      "File number 42 added to dataframe\n",
      "File number 43 added to dataframe\n",
      "File number 44 added to dataframe\n",
      "File number 45 added to dataframe\n",
      "File number 46 added to dataframe\n",
      "File number 47 added to dataframe\n",
      "File number 48 added to dataframe\n",
      "File number 49 added to dataframe\n",
      "File number 50 added to dataframe\n",
      "File number 51 added to dataframe\n",
      "File number 52 added to dataframe\n",
      "File number 53 added to dataframe\n",
      "File number 54 added to dataframe\n",
      "File number 55 added to dataframe\n",
      "File number 56 added to dataframe\n",
      "File import done\n",
      "json columns extracted\n",
      "target and features split is done\n",
      "Training set has 161528 samples.\n",
      "Testing set has 40383 samples.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MAIN SCRIPT =============================================================\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    # this script loads data and performs all manipulations in order to get it ready for use in a model\n",
    "    # Read data\n",
    "\n",
    "    df = load_kickstarter_data('kickstarter/data')\n",
    "    \n",
    "    # Extract category data from json\n",
    "    df = extract_json_data(df)\n",
    "    \n",
    "    # drop campaigns that are still ongoing\n",
    "    df = df[df.state != 'live'] # change this to sucessful and \n",
    "    \n",
    "    # convert unix timestamps and calculate campaign duration\n",
    "    df = get_duration(df)\n",
    "    \n",
    "    # Get blurb length\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    #create goal data in single currency\n",
    "    df = currency_conversion(df)\n",
    "    \n",
    "    # encode target variable 'state' to numerical values, success is 1 all others are fail and 0\n",
    "    df = get_target(df,target='state', new_target_var='success', success_label='successful')\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    df=drop_columns(df)\n",
    "    df.head()\n",
    "       \n",
    "    # Split the data into features and target label\n",
    "    target, features = get_target_and_features(df)\n",
    "\n",
    "    # split categorical columns into dummies\n",
    "    features = pd.get_dummies(features, columns=['country', 'category_main','category_sub'], drop_first=True) #Avoid dummy trap   \n",
    "\n",
    "    # address skew    \n",
    "    num_columns = ['project_duration_days', 'blurb_length', 'usd_goal']\n",
    "    features = fix_skew(features, skewed=num_columns)\n",
    " \n",
    "    # scale numerical features\n",
    "    features = scale_features(features, num_columns)\n",
    "    \n",
    "     # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = test_train_split_kickstarter(features, target, test_size = 0.2, random_state = RSEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['project_duration_days', 'blurb_length', 'usd_goal', 'country_AU',\n",
       "       'country_BE', 'country_CA', 'country_CH', 'country_DE', 'country_DK',\n",
       "       'country_ES',\n",
       "       ...\n",
       "       'category_sub_wearables', 'category_sub_weaving', 'category_sub_web',\n",
       "       'category_sub_webcomics', 'category_sub_webseries',\n",
       "       'category_sub_woodworking', 'category_sub_workshops',\n",
       "       'category_sub_world music', 'category_sub_young adult',\n",
       "       'category_sub_zines'],\n",
       "      dtype='object', length=181)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (161528, 181)\n",
      "X_test.shape: (40383, 181)\n",
      "y_train.shape: (161528,)\n",
      "y_test.shape: (40383,)\n",
      "y_pred.shape: (40383,)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of dataframes\n",
    "print('X_train.shape:',X_train.shape)\n",
    "print('X_test.shape:',X_test.shape)\n",
    "print('y_train.shape:',y_train.shape)\n",
    "print('y_test.shape:',y_test.shape)\n",
    "print('y_pred.shape:',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_decsion_tree(X_train, X_test, y_train, y_test):\n",
    "    '''This is the first model to set a baseline prediction'''\n",
    "    # Fit a simple decision tree first\n",
    "    clf_gini = train_using_gini(X_train, y_train) \n",
    "    \n",
    "    # Create predictions using simple model - decision tree\n",
    "    y_pred = prediction(X_test, clf_object=clf_gini)\n",
    "    \n",
    "    # show results\n",
    "    cal_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SVC model from sklearn\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVM_model_tiral(X_train, y_train, X_test, y_test):\n",
    "    ''' This function tries a Support Vector Classifier to see if we can handle the data volume'''\n",
    "    # we will try it out on 3 small subsets and compare to simple decision tree.\n",
    "    svm_trial = SVC(random_state = RSEED)\n",
    "\n",
    "    # TODO: Calculate the number of samples for 1%, 5%, and 10% of the training data\n",
    "    samples_1 = int(round(len(X_train) / 100))\n",
    "    samples_5 = int(round(len(X_train) / 20))\n",
    "    samples_10 = int(round(len(X_train) / 10))\n",
    "\n",
    "    # Collect results on the learners\n",
    "    results = {}\n",
    "    for clf in [svm_trial]:\n",
    "        clf_name = svm_trial.__class__.__name__\n",
    "        results[clf_name] = {}\n",
    "        for i, samples in enumerate([samples_1, samples_5, samples_10]):\n",
    "            results[clf_name][i] = \\\n",
    "            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "    for i in results.items():\n",
    "        print (i[0])\n",
    "        display(pd.DataFrame(i[1]).rename(columns={0:'1%', 1:'5%', 2:'10%'}))\n",
    "# Results show that it is already very slow on 1% and 5% of data\n",
    "# and the results are not promising so we will try other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashed out because it takes so long\n",
    "#SVM_model_tiral(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def complex_model_selection(X_train, y_train, X_test, y_test):\n",
    "    '''This function runs three models to determine which model is the best candidate for further review'''\n",
    "    # Initialize the three models, the random states are set to RSEED so we know how to reproduce the model later\n",
    "    clf_A = DecisionTreeClassifier(random_state=RSEED)\n",
    "    clf_B = RandomForestClassifier(random_state = RSEED, n_jobs=-1)\n",
    "    clf_C = AdaBoostClassifier(random_state = RSEED)\n",
    "\n",
    "    # Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "    samples_1 = int(round(len(X_train) / 100))\n",
    "    samples_10 = int(round(len(X_train) / 10))\n",
    "    samples_100 = int(round(len(X_train) / 1))\n",
    "\n",
    "    # Collect results on the learners\n",
    "    results = {}\n",
    "    for clf in [clf_A, clf_B, clf_C]:\n",
    "        clf_name = clf.__class__.__name__\n",
    "        results[clf_name] = {}\n",
    "        for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "            results[clf_name][i] = \\\n",
    "            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "    # Printing out the values\n",
    "    for i in results.items():\n",
    "        print (i[0])\n",
    "        display(pd.DataFrame(i[1]).rename(columns={0:'1%', 1:'10%', 2:'100%'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier trained on 1615 samples.\n",
      "DecisionTreeClassifier trained on 16153 samples.\n",
      "DecisionTreeClassifier trained on 161528 samples.\n",
      "RandomForestClassifier trained on 1615 samples.\n",
      "RandomForestClassifier trained on 16153 samples.\n",
      "RandomForestClassifier trained on 161528 samples.\n",
      "AdaBoostClassifier trained on 1615 samples.\n",
      "AdaBoostClassifier trained on 16153 samples.\n",
      "AdaBoostClassifier trained on 161528 samples.\n",
      "DecisionTreeClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>0.033539</td>\n",
       "      <td>0.441925</td>\n",
       "      <td>7.688792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>0.147962</td>\n",
       "      <td>0.057216</td>\n",
       "      <td>0.060279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_test</th>\n",
       "      <td>0.641532</td>\n",
       "      <td>0.689597</td>\n",
       "      <td>0.751306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998865</td>\n",
       "      <td>0.988701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_test</th>\n",
       "      <td>0.690084</td>\n",
       "      <td>0.732305</td>\n",
       "      <td>0.781459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1%       10%      100%\n",
       "train_time  0.033539  0.441925  7.688792\n",
       "pred_time   0.147962  0.057216  0.060279\n",
       "acc_train   1.000000  0.996667  0.986667\n",
       "acc_test    0.641532  0.689597  0.751306\n",
       "f_train     1.000000  0.998865  0.988701\n",
       "f_test      0.690084  0.732305  0.781459"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>0.198509</td>\n",
       "      <td>0.865281</td>\n",
       "      <td>17.196880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>0.217174</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.530815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_test</th>\n",
       "      <td>0.714013</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.789565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_test</th>\n",
       "      <td>0.750985</td>\n",
       "      <td>0.776522</td>\n",
       "      <td>0.813041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1%       10%       100%\n",
       "train_time  0.198509  0.865281  17.196880\n",
       "pred_time   0.217174  0.273195   0.530815\n",
       "acc_train   1.000000  1.000000   0.983333\n",
       "acc_test    0.714013  0.742268   0.789565\n",
       "f_train     1.000000  1.000000   0.981048\n",
       "f_test      0.750985  0.776522   0.813041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>0.159644</td>\n",
       "      <td>1.252806</td>\n",
       "      <td>15.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>0.782230</td>\n",
       "      <td>0.791573</td>\n",
       "      <td>0.794514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_train</th>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_test</th>\n",
       "      <td>0.714855</td>\n",
       "      <td>0.729589</td>\n",
       "      <td>0.737043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_train</th>\n",
       "      <td>0.825190</td>\n",
       "      <td>0.801938</td>\n",
       "      <td>0.764263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_test</th>\n",
       "      <td>0.751933</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>0.772646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1%       10%       100%\n",
       "train_time  0.159644  1.252806  15.000945\n",
       "pred_time   0.782230  0.791573   0.794514\n",
       "acc_train   0.803333  0.776667   0.730000\n",
       "acc_test    0.714855  0.729589   0.737043\n",
       "f_train     0.825190  0.801938   0.764263\n",
       "f_test      0.751933  0.762300   0.772646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "complex_model_selection(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df['success'] = df.state.apply(lambda x: True if x == 'successful' else False)\n",
    "df['match'] = df['success'] == df['reached_goal']\n",
    "df.query('state == \"successful\"').match.value_counts()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff for figureing out exchange rates\n",
    "'''df['implied_fx_rate'] = df['usd_pledged'] / df['pledged']\n",
    "df[['usd_pledged','implied_fx_rate','fx_rate','static_usd_rate']].head(10)\n",
    "df['usd_goal'] = df['goal'] * df['static_usd_rate']\n",
    "df['reached_goal'] = df['usd_goal'] < df['usd_pledged']\n",
    "df['success'] = df.state.apply(lambda x: True if x == 'successful' else False)\n",
    "df['match'] = df['success'] == df['reached_goal']\n",
    "df.query('state == \"successful\" & usd_type == \"international\"').match.value_counts()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf_2nd_project] *",
   "language": "python",
   "name": "conda-env-nf_2nd_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
